{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4556199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1114423",
   "metadata": {},
   "source": [
    "# i. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8d4d8",
   "metadata": {},
   "source": [
    "### i.i Explain Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3292c2",
   "metadata": {},
   "source": [
    "**Linear regression** is a type of supervised machine learning algorithm that is commonly used for regression problems. **The goal of Linear Regression is to model a dataset by finding a line that best fits the linear relationship between the independent variables and the dependent variable.**\n",
    "\n",
    "Linear regression is a type of **parametric regression**, **which means that it makes certain assumptions about the distribution of the data and the relationship between the independent and dependent variables**. For example, it assumes that the relationship between the independent and dependent variables is linear, and that the errors (or residuals) are normally distributed and have constant variance.\n",
    "\n",
    "There are two parameters in Linear Regression: the **slope** and the **intercept** of the line (y = mx + c), which can be adjusted to find the best-fitted line. In addition, we have a **cost function** that is **used to determine how well the line fits the data**. Most commonly used cost function: MSE, calculates the average squared distances between the predicted and actual values. \n",
    "\n",
    "The aim here is to find the **optimal coefficients** for the best-fitting line. That is to say the global minima of the curve that we get between these parameteres and the cost function. The **optimization technique** to minimize the cost function is called **Gradient Desceent**. And since we're not gonna find these values -- the values of slope and intercept -- by trial and experimentation, we've got **convergence algorithm** for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0c34b",
   "metadata": {},
   "source": [
    "### i.ii What are the basic assumptions that are to be made in the linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cf7a5",
   "metadata": {},
   "source": [
    "1. **Linearity:** The relationship between the independent and dependent variables should be linear.<br><br>\n",
    "    \n",
    "2. **Homoschedasticity:** Error terms (residuals) must show constant variance across all levels of independent variables. This means that the spread of the residuals should be roughly the same at all values of the independent variable.\n",
    "\n",
    "    **This is important because if the residuals have different variances at different levels of the predictor variables, it can lead to inaccurate estimates of the regression coefficients and make it harder to assess the goodness of fit of the model.**\n",
    "    \n",
    "    It can be checked by a **residual plot** -- *a scatter plot of residuals against the predicted values.*<br><br>\n",
    "\n",
    "3. **Independence of Residuals:** The errors (or residuals) are independent of each other, meaning that the error for one observation is not related to the error for another observation.<br><br>\n",
    "\n",
    "4. **Normality of Residuals:**  The errors (or residuals) should be normally distributed. This means that the distribution of the residuals should be symmetric and bell-shaped.<br>Additionally, normality of residuals is important for hypothesis testing and confidence interval estimation. If the residuals are normally distributed, then the t-tests and confidence intervals can be calculated using standard normal distribution tables. \n",
    "\n",
    "    **Why?** Because this'd mean that the distribution of the errors is centered around zero, and the majority of errors are small with fewer large errors.\n",
    "<br><br>\n",
    "\n",
    "5. **Mean of Residuals** should be **zero**.<br>This means that on average, the model's predictions should be equal to the true values of the dependent variable. If the mean of the residuals is not zero, then it indicates that there may be a systematic bias or error in the model's predictions, which can lead to inaccurate results<br><br>\n",
    "\n",
    "6. **No Mulitcolliearity:** There shouldn't be any sort of correlation between the independent variables.<br><br>\n",
    "\n",
    "7. **Exogeneity:** There is no correlation between the independent variables and the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34defddc",
   "metadata": {},
   "source": [
    "### i.iii Is Feature Scaling required herein Linear Regeression. Why/Why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27464028",
   "metadata": {},
   "source": [
    "The goal in the linear regression is to model a dataset (assuming there's a linear relationship between the dependent and independent variables) by a linear line which is controlled by two parameters -- slope and intercept. Now to attain this goal, we have to make sure that we've optimal values of slope and intercept, and to find them we've have a technique called **gradient descent**. It involves plotting a curve between one parameter and the cost function at a time and finding the **global mininma** -- the point where the errors are least.\n",
    "\n",
    "**Reason:** When the features in the dataset have different scales, the gradient descent algorithm may take longer to converge to the global minima. This is because the steps taken by the algorithm will be larger for the features with larger scales, which can cause it to overshoot the minimum and take longer to converge. **Thus, it's essential to apply feature scaling to scale the features to ensure that the gradient descent algorithm converges faster and more accurately.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e983df1",
   "metadata": {},
   "source": [
    "### i.iv What's the impact of missing values and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95d2e7",
   "metadata": {},
   "source": [
    "- ### Impact of Outliers:\n",
    "\n",
    "    Outliers can have a significant impact on the regression line and can cause the model to be less accurate in predicting new data points. This is because the **regression line is influenced by these extreme points, leading to a less representative model**. Therefore, it's important to identify and address outliers before building a linear regression model to ensure accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad04b3c",
   "metadata": {},
   "source": [
    "- ### Impact of Missing Values:\n",
    "    \n",
    "  Missing values in the dataset **can lead to a decrease in the number of observations available for analysis** and **may cause bias in the results of linear regression**. While the amount of missing data is small, it may be possible to delete the rows with missing data without significantly affecting the results. However, if the amount of missing data is large, it may be necessary to impute the missing values using methods such as mean imputation, regression imputation, or multiple imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a011c271",
   "metadata": {},
   "source": [
    "### i.v Can impact of outliers be mitigated should you use MAE as your cost function in Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b58739",
   "metadata": {},
   "source": [
    "Using **Mean Absolute Error (MAE)** as a cost function can help reduce the impact of outliers in linear regression. Since, unlike Mean Squared Error (MSE) which squares the differences between predicted and actual values, MAE takes the absolute differences, which makes **MAE less sensitive to outliers because it does not give them as much weight as MSE does**.\n",
    "\n",
    "**NOTE:** However, it's important to note that **using MAE instead of MSE may result in a less accurate model in cases where there are few outliers**. This is **because MAE does not differentiate between small and large errors as MSE does**. So, while MAE can be a good choice when dealing with datasets that have a significant number of outliers, it may not always be the best choice depending on the specific characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d087e",
   "metadata": {},
   "source": [
    "### i.vi State the advantages and disadvantages of the Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef91105",
   "metadata": {},
   "source": [
    "- ### Advantages:\n",
    "\n",
    "    1. Behaves exceptionally well when there is a linear relation between the independent and dependent variables.<br><br>\n",
    "\n",
    "    2. **Efficieny:** Linear regression **can be computed efficiently**, even with large datasets, making it suitable for real-time applications.<br><br>\n",
    "\n",
    "    3. Can handle **Overfitting** by using techniques such as dimensionlity reduction, cross-validation. and regularization.<br><br>\n",
    "\n",
    "    4. **Simplicity:** Simple and easy to understand and interpret.<br><br>\n",
    "\n",
    "    5. **Easy Implementation:** Linear regression is a simple and straightforward algorithm that can be easily implemented in most programming languages.<br><br>\n",
    "\n",
    "    6. **Versatality:** Linear regression can be applied to a wide range of problems, including prediction, classification (logistic regression), and feature selection (LASSO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b6a84",
   "metadata": {},
   "source": [
    "- ### Disadvantages:\n",
    "\n",
    "  1. **Limited to Linear Relationships:** Linear regression is limited to finding linear relationships between the independent and dependent variables. It may not perform well if the true relationship between the variables is nonlinear.<br><br>\n",
    "  \n",
    "  2. **Sensitive to Outliers:** The regression line can be (and usually does get) influenced by the extreme points of the dataset leading to a model that is not true representative of the dataset.<br><br>\n",
    "  \n",
    "  3. **Prone to Overfitting:** Linear regression is prone to overfitting, **particularly when dealing with high-dimensional datasets or when the number of features exceeds the number of observations**.<br><br>\n",
    "  \n",
    "  4. **Not Suitable for Categorical or Binary Outcomes:** Linear regression is not well-suited for categorical or binary outcomes. Other methods, such as logistic regression or decision trees, may be more appropriate in such cases.<br><br>\n",
    "\n",
    "  5. **Can be Affected by Multicollinearity:** Multicollinearity between the independent variables can adversely affect the performance of linear regression **by inflating the standard errors of the coefficients** and **reducing the precision of the estimates**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec6fe0",
   "metadata": {},
   "source": [
    "### i.vii How's multicollinearity a problem for Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0661f8",
   "metadata": {},
   "source": [
    "When two or more predictor variables are highly correlated, **it becomes challenging to discern the effect of each variable on the dependent variable**. **This high correlation leads to an unstable and unreliable estimation of coefficients**, which can lead to incorrect conclusions about the relationships between the dependent and independent variables. \n",
    "\n",
    "By making the multicollinearity assumption, we ensure that the model's coefficients are stable, and we can interpret them reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb05658",
   "metadata": {},
   "source": [
    "### i.viii Explain the Convergence Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39e3cc",
   "metadata": {},
   "source": [
    "An algorithm that automates the process of converging towards the global minima where the error is the least. In linear regression, the most commonly used algorithm for convergence is **gradient descent**. \n",
    "\n",
    "**Gradient Descent** is an **iterative algorithm that repeatedly adjusts the values of the parameters (slope and intercept) until it finds the optimal values that minimize the cost function**.\n",
    "\n",
    "Gradient descent starts with an **initial guess** of the parameter values and then **repeatedly updates them using the gradient of the cost function with respect to the parameters**. The gradient points in the direction of the steepest increase of the cost function, so the algorithm adjusts the parameters in the opposite direction of the gradient to minimize the cost.\n",
    "\n",
    "The algorithm continues to update the parameters until it converges, meaning that the updates become smaller and smaller, indicating that the algorithm has found the best values of the parameters. **The `learning rate` is an important hyperparameter that controls the step size of the parameter updates and can affect the convergence of the algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2c5c6",
   "metadata": {},
   "source": [
    "# ii. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de05aa",
   "metadata": {},
   "source": [
    "## ii.i L2 Regularization (Ridge Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e2349",
   "metadata": {},
   "source": [
    "L2 regularization is a **technique used in machine learning to prevent overfitting**. Overfitting occurs when a model learns the training data too well, to the point that it begins to memorize the training data instead of generalizing to new, unseen data.\n",
    "\n",
    "**L2 regularization addresses this problem by adding a penalty term to the original cost function that penalizes large weights in the model. The `penalty term` is proportional to the `square of the magnitude of the weights`, and the regularization strength is controlled by a hyperparameter called `lambda`.** By increasing the value of lambda, the penalty for large weights increases, and the model is encouraged to use smaller weights.\n",
    "\n",
    "This has the effect of simplifying the model and reducing overfitting. **During training, the optimizer tries to minimize the cost function, which now includes the regularization term. As a result, the weights are updated to minimize the sum of the original cost function and the regularization term. This pushes the weights towards smaller values, reducing overfitting.**\n",
    "\n",
    "So, in summary, L2 regularization adds a penalty term to the cost function, which encourages the model to use smaller weights, reducing overfitting and improving generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d4eef",
   "metadata": {},
   "source": [
    "### ii.i.i New Cost Function\n",
    "\n",
    "The cost function used in L2 regularization is typically called the **regularized cost function** or the **L2 cost function**. It is obtained by adding a regularization term to the original cost function, which is typically the mean squared error (MSE) for regression problems or the cross-entropy loss for classification problems.\n",
    "\n",
    "For a regression problem, the regularized cost function with L2 regularization can be written as:\n",
    "\n",
    "**`J(w) = 1/2m * (sum of squared errors) + λ/2m * (sum of squared weights)`**\n",
    "\n",
    "where:\n",
    "\n",
    "`w`: the weight vector\n",
    "\n",
    "`m`: the number of training examples\n",
    "\n",
    "`λ`: the regularization parameter (also called the regularization strength or the L2 penalty)\n",
    "`(sum of squared errors)`: the sum of the squared differences between the predicted and actual values for all training examples.\n",
    "\n",
    "**`(sum of squared weights)`: the sum of the squared weights in the model.**\n",
    "\n",
    "\n",
    "The first term in the cost function measures the difference between the predicted and actual values, while the second term penalizes large weights. The λ/2m factor determines the strength of the regularization, where a larger λ leads to more regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50706b",
   "metadata": {},
   "source": [
    "## ii.ii L1 Regularization (LASSO Regression)\n",
    "\n",
    "Least Absolute Shrinkage & Selection Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f41e52",
   "metadata": {},
   "source": [
    "L1 regularization, also known as Lasso regularization, is again a technique used in machine learning to prevent overfitting of a model. In addition, **it can also be used for feature selection**.\n",
    "\n",
    "**L1 regularization works by adding a penalty term to the model's cost function, which discourages the model from using too many features or variables in its predictions.** This penalty term is calculated by taking the sum of the absolute values of the model's coefficients, which are the **numerical values that indicate the strength and direction of each feature's impact on the prediction**.\n",
    "\n",
    "In simpler terms, L1 regularization is like a \"budget\" for the model, where it can only use a limited number of features to make its predictions. This encourages the model to focus on the most important features and disregard the less important ones, leading to a more accurate prediction of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ddee2d",
   "metadata": {},
   "source": [
    "### ii.ii.i Feature Selection? How?\n",
    "\n",
    "**Penalty term** being added to the model's cost function **encourages the model to reduce the impact of irrelevant features on its predictions**.\n",
    "\n",
    "The penalty term is calculated as the **sum of the absolute values of the model's coefficients**, which means that it will be larger when the coefficients are larger. **Since the coefficients represent the strength and direction of each feature's impact on the prediction, L1 regularization encourages the model to set the coefficients of the irrelevant features to zero or near-zero.**\n",
    "\n",
    "**When the coefficients of irrelevant features are set to zero, they are effectively removed from the model, which reduces the complexity of the model and improves its performance by preventing overfitting.** This process is also called \"feature selection\", as it helps the model to identify and focus on the most relevant features for its predictions.\n",
    "\n",
    "In summary, L1 regularization helps in feature selection by adding a penalty term that encourages the model to reduce the impact of irrelevant features on its predictions, leading to a simpler and more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16806efe",
   "metadata": {},
   "source": [
    "### Example: \n",
    "\n",
    "Say, we have a dataset with two features, \"age\" and \"income\", and we want to predict whether a person will buy a certain product or not based on these features. We build a logistic regression model on this dataset, without any regularization.\n",
    "\n",
    "Initially, the model may use both \"age\" and \"income\" to make its predictions. However, if the dataset is small or noisy, the model may overfit and give too much weight to one or both features, leading to poor performance on new data.\n",
    "\n",
    "To avoid overfitting, we can apply L1 regularization to the model. **The regularization term will add a penalty based on the sum of the absolute values of the coefficients. The larger the coefficient, the larger the penalty.**\n",
    "\n",
    "Let's say that after applying L1 regularization, the model assigns a smaller coefficient to \"income\" compared to \"age\". The penalty term encourages the model to set the coefficient of \"income\" to zero or near-zero, since it's not as important for the prediction as \"age\". This is an example of feature selection.\n",
    "\n",
    "By setting the coefficient of \"income\" to zero or near-zero, the model is effectively removing this feature from the prediction, as if it wasn't in the dataset. This results in a simpler model that is less likely to overfit and performs better on new data.\n",
    "\n",
    "In summary, L1 regularization helps in feature selection by adding a penalty term that encourages the model to reduce the impact of irrelevant features on its predictions, leading to a simpler and more accurate model. In our example, L1 regularization helped to identify and remove the less important feature \"income\" from the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f12bc",
   "metadata": {},
   "source": [
    "## ii.iii Lambda\n",
    "\n",
    "**Lambda** is a hyperparameter in both L1 and L2 regularization techniques that **controls the strength of the penalty term in the model's cost function**. **The value of lambda determines how much the model is penalized for having large coefficients.**\n",
    "\n",
    "In simpler terms, lambda acts like a \"fine\" on the model for using too many or too large features. A higher value of lambda means a higher fine, which encourages the model to reduce the impact of less important features and make its predictions more generalizable to new data.\n",
    "\n",
    "- In `L1 regularization`, **a higher value of lambda leads to more coefficients being set to zero, as the penalty term becomes larger and encourages the model to remove less important features from the prediction.** This leads to a simpler and more interpretable model, but it may also result in a loss of accuracy if important features are mistakenly removed.<br><br>\n",
    "\n",
    "- In `L2 regularization`, **a higher value of lambda leads to smaller coefficients for all features, rather than setting some coefficients to zero.** This reduces the overall complexity of the model and improves its performance on new data by reducing overfitting.\n",
    "\n",
    "In summary, lambda is a hyperparameter that controls the strength of the penalty term in L1 and L2 regularization techniques. A higher value of lambda encourages the model to reduce the impact of less important features, resulting in a simpler and more generalizable model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
