{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4556199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1114423",
   "metadata": {},
   "source": [
    "# i. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8d4d8",
   "metadata": {},
   "source": [
    "### i.i Explain Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3292c2",
   "metadata": {},
   "source": [
    "**Linear regression** is a type of supervised machine learning algorithm that is commonly used for regression problems. **The goal of Linear Regression is to model a dataset by finding a line that best fits the linear relationship between the independent variables and the dependent variable.**\n",
    "\n",
    "Linear regression is a type of **parametric regression**, **which means that it makes certain assumptions about the distribution of the data and the relationship between the independent and dependent variables**. For example, it assumes that the relationship between the independent and dependent variables is linear, and that the errors (or residuals) are normally distributed and have constant variance.\n",
    "\n",
    "There are two parameters in Linear Regression: the **slope** and the **intercept** of the line (y = mx + c), which can be adjusted to find the best-fitted line. In addition, we have a **cost function** that is **used to determine how well the line fits the data**. Most commonly used cost function: MSE, calculates the average squared distances between the predicted and actual values. \n",
    "\n",
    "The aim here is to find the **optimal coefficients** for the best-fitting line. That is to say the global minima of the curve that we get between these parameteres and the cost function. The **optimization technique** to minimize the cost function is called **Gradient Desceent**. And since we're not gonna find these values -- the values of slope and intercept -- by trial and experimentation, we've got **convergence algorithm** for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0c34b",
   "metadata": {},
   "source": [
    "### i.ii What are the basic assumptions that are to be made in the linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cf7a5",
   "metadata": {},
   "source": [
    "1. **Linearity:** The relationship between the independent and dependent variables should be linear.<br><br>\n",
    "    \n",
    "2. **Homoschedasticity:** Error terms (residuals) must show constant variance across all levels of independent variables. This means that the spread of the residuals should be roughly the same at all values of the independent variable.\n",
    "\n",
    "    **Why?** Beacuse this'd mean that the distribution of the errors is centered around zero, and the majority of errors are small with fewer large errors.\n",
    "    \n",
    "    It can be checked by a **residual plot** -- *a scatter plot of residuals against the predicted values.*<br><br>\n",
    "\n",
    "3. **Independence of Residuals:** The errors (or residuals) are independent of each other, meaning that the error for one observation is not related to the error for another observation.<br><br>\n",
    "\n",
    "4. **Normality of Residuals:**  The errors (or residuals) should be normally distributed. This means that the distribution of the residuals should be symmetric and bell-shaped.<br>Additionally, normality of residuals is important for hypothesis testing and confidence interval estimation. If the residuals are normally distributed, then the t-tests and confidence intervals can be calculated using standard normal distribution tables. <br><br>\n",
    "\n",
    "5. **Mean of Residuals** should be **zero**.<br>This means that on average, the model's predictions should be equal to the true values of the dependent variable. If the mean of the residuals is not zero, then it indicates that there may be a systematic bias or error in the model's predictions, which can lead to inaccurate results<br><br>\n",
    "\n",
    "6. **No Mulitcolliearity:** There shouldn't be any sort of correlation between the independent variables.<br><br>\n",
    "\n",
    "7. **Exogeneity:** There is no correlation between the independent variables and the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34defddc",
   "metadata": {},
   "source": [
    "### i.iii Is Feature Scaling required herein Linear Regeression. Why/Why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27464028",
   "metadata": {},
   "source": [
    "The goal in the linear regression is to model a dataset (assuming there's a linear relationship between the dependent and independent variables) by a linear line which is controlled by two parameters -- slope and intercept. Now to attain this goal, we have to make sure that we've optimal values of slope and intercept, and to find them we've have a technique called **gradient descent**. It involves plotting a curve between one parameter and the cost function at a time and finding the **global mininma** -- the point where the errors are least.\n",
    "\n",
    "When the features in the dataset have different scales, the gradient descent algorithm may take longer to converge to the global minima. This is because the steps taken by the algorithm will be larger for the features with larger scales, which can cause it to overshoot the minimum and take longer to converge. **Thus, it's essential to apply feature scaling to scale the features to ensure that the gradient descent algorithm converges faster and more accurately.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e983df1",
   "metadata": {},
   "source": [
    "### i.iv What's the impact of missing values and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95d2e7",
   "metadata": {},
   "source": [
    "- ### Impact of Outliers:\n",
    "\n",
    "    Outliers can have a significant impact on the regression line and can cause the model to be less accurate in predicting new data points. This is because the **regression line is influenced by these extreme points, leading to a less representative model**. Therefore, it's important to identify and address outliers before building a linear regression model to ensure accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad04b3c",
   "metadata": {},
   "source": [
    "- ### Impact of Missing Values:\n",
    "    \n",
    "  Missing values in the dataset can lead to a decrease in the number of observations available for analysis and **may cause bias in the results of linear regression**. While the amount of missing data is small, it may be possible to delete the rows with missing data without significantly affecting the results. However, if the amount of missing data is large, it may be necessary to impute the missing values using methods such as mean imputation, regression imputation, or multiple imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a011c271",
   "metadata": {},
   "source": [
    "### i.v Can impact of outliers be mitigated should you use MAE as your cost function in Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b58739",
   "metadata": {},
   "source": [
    "Using **Mean Absolute Error (MAE)** as a cost function can help reduce the impact of outliers in linear regression. Since, unlike Mean Squared Error (MSE) which squares the differences between predicted and actual values, MAE takes the absolute differences, it makes **MAE less sensitive to outliers because it does not give them as much weight as MSE does**.\n",
    "\n",
    "**NOTE:** However, it's important to note that **using MAE instead of MSE may result in a less accurate model in cases where there are few outliers**. This is **because MAE does not differentiate between small and large errors as MSE does**. So, while MAE can be a good choice when dealing with datasets that have a significant number of outliers, it may not always be the best choice depending on the specific characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d087e",
   "metadata": {},
   "source": [
    "### i.vi State the advantages and disadvantages of the Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef91105",
   "metadata": {},
   "source": [
    "- ### Advantages:\n",
    "\n",
    "    1. Behaves exceptionally well when there is a linear relation between the independent and dependent variables.<br><br>\n",
    "\n",
    "    2. **Efficieny:** Linear regression **can be computed efficiently**, even with large datasets, making it suitable for real-time applications.<br><br>\n",
    "\n",
    "    3. Can handle **Overfitting** by using techniques such as dimensionlity reduction, cross-validation. and regularization.<br><br>\n",
    "\n",
    "    4. **Simplicity:** Simple and easy to understand and interpret.<br><br>\n",
    "\n",
    "    5. **Easy Implementation:** Linear regression is a simple and straightforward algorithm that can be easily implemented in most programming languages.<br><br>\n",
    "\n",
    "    6. **Versatality:** Linear regression can be applied to a wide range of problems, including prediction, classification (logistic regression), and feature selection (LASSO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b6a84",
   "metadata": {},
   "source": [
    "- ### Disadvantages:\n",
    "\n",
    "  1. **Limited to Linear Relationships:** Linear regression is limited to finding linear relationships between the independent and dependent variables. It may not perform well if the true relationship between the variables is nonlinear.<br><br>\n",
    "  \n",
    "  2. **Sensitive to Outliers:** The regression line can be (and usually does get) influenced by the extreme points of the dataset leading to a model that is not true representative of the dataset.<br><br>\n",
    "  \n",
    "  3. **Prone to Overfitting:** Linear regression is prone to overfitting, **particularly when dealing with high-dimensional datasets or when the number of features exceeds the number of observations**.<br><br>\n",
    "  \n",
    "  4. **Not Suitable for Categorical or Binary Outcomes:** Linear regression is not well-suited for categorical or binary outcomes. Other methods, such as logistic regression or decision trees, may be more appropriate in such cases.<br><br>\n",
    "\n",
    "  5. **Can be Affected by Multicollinearity:** Multicollinearity between the independent variables can adversely affect the performance of linear regression **by inflating the standard errors of the coefficients** and **reducing the precision of the estimates**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec6fe0",
   "metadata": {},
   "source": [
    "### i.vii How's mulitcollinearity a problem for Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0661f8",
   "metadata": {},
   "source": [
    "When two or more predictor variables are highly correlated, **it becomes challenging to discern the effect of each variable on the dependent variable**. **This high correlation leads to an unstable and unreliable estimation of coefficients**, which can lead to incorrect conclusions about the relationships between the dependent and independent variables. \n",
    "\n",
    "By making the multicollinearity assumption, we ensure that the model's coefficients are stable, and we can interpret them reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb05658",
   "metadata": {},
   "source": [
    "### i.viii Explain the Convergence Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39e3cc",
   "metadata": {},
   "source": [
    "An algorithm that automates the process of converging towards the global minima where the error is the least. In linear regression, the most commonly used algorithm for convergence is **gradient descent**. \n",
    "\n",
    "**Gradient Descent** is an **iterative algorithm that repeatedly adjusts the values of the parameters (slope and intercept) until it finds the optimal values that minimize the cost function**.\n",
    "\n",
    "Gradient descent starts with an **initial guess** of the parameter values and then **repeatedly updates them using the gradient of the cost function with respect to the parameters**. The gradient points in the direction of the steepest increase of the cost function, so the algorithm adjusts the parameters in the opposite direction of the gradient to minimize the cost.\n",
    "\n",
    "The algorithm continues to update the parameters until it converges, meaning that the updates become smaller and smaller, indicating that the algorithm has found the best values of the parameters. **The `learning rate` is an important hyperparameter that controls the step size of the parameter updates and can affect the convergence of the algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2c5c6",
   "metadata": {},
   "source": [
    "# ii. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3918854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
