{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f86b04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9effc",
   "metadata": {},
   "source": [
    "### i. Why Naive Bayes is Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b6117",
   "metadata": {},
   "source": [
    "- Naive Bayes is a popular **simple** and **effective** machine-learning algorithm primarily used for classification and more specifically, **its forte lies in text classification tasks** -- **spam filtering**, **sentiment analysis**, etc -- particularly in Natural Language Processing.\n",
    "\n",
    "\n",
    "- Unlike other sophisticated machine learning algorithms which mostly rely on calculation of eucledian distances, it is entirely based on the **Bayes' theorem**, which states that the probability of a hypothesis (in this case, a class label) given some observed evidence (in this case, a set of features) is proportional to the probability of the evidence given the hypothesis, **multiplied by the prior probability of the hypothesis**, and thus calculating probabilities it's all about.\n",
    "\n",
    "\n",
    "- **ASSUMPTION:** **Naive Bayes assumes that the features are conditionally independent given the class label, meaning that the presence or absence of one feature does not affect the probability of any other feature.** And as such, it trades high bias (due to conditional independence of features given the class label) for low variance (will generalize better).\n",
    "\n",
    "\n",
    "- Naive Bayes can handle both binary and continuous features, but it requires a probability distribution over the feature values. For binary features, the distribution is a Bernoulli distribution, while for continuous features, it is usually assumed to be a Gaussian distribution.\n",
    "\n",
    "\n",
    "- There are **three main types** of Naive Bayes classifiers: **Gaussian Naive Bayes**, **Multinomial Naive Bayes**, and **Bernoulli Naive Bayes**. Gaussian Naive Bayes assumes that the continuous features are normally distributed, while Multinomial and Bernoulli Naive Bayes are used for discrete data such as text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601611d",
   "metadata": {},
   "source": [
    "### ii. What's the basic difference between Bernoulli's Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc55975",
   "metadata": {},
   "source": [
    "The **basic difference** between Bernoulli's Naive Bayes and Multinomial Naive Bayes is the **type of data they are designed to handle.**\n",
    "\n",
    "**`Bernoulli's Naive Bayes`** is designed for **binary or Boolean features**, where each feature can take on one of two possible values, usually 0 or 1. It is commonly used for text classification tasks where the presence or absence of a word in a document is used as a feature.\n",
    "\n",
    "On the other hand, **`Multinomial Naive Bayes`** is designed for **count-based data**, **where the features represent the frequency of occurrence of a particular event or item**. It is commonly used for text classification tasks where the frequency of occurrence of each word in a document is used as a feature.\n",
    "\n",
    "\n",
    "Both models assume independence between the features, given the class variable, and use Bayes' theorem to calculate the probability of a document belonging to a particular class based on the probabilities of each feature given that class. **The main difference in the algorithm lies in the probability calculations for each feature, with Bernoulli's Naive Bayes using binary probabilities and Multinomial Naive Bayes using frequency-based probabilities.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9cc54",
   "metadata": {},
   "source": [
    "### iii. State any disadvantages of using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25ef32",
   "metadata": {},
   "source": [
    "One important consideration when using Naive Bayes is the **issue of imbalanced classes**. If one class has a much larger number of examples than the other class, the classifier may be biased towards the larger class. In such cases, techniques such as oversampling or undersampling can be used to balance the classes.\n",
    "\n",
    "- **Independence assumption:** Naive Bayes assumes that all features are independent of each other given the class. **This assumption is often unrealistic** in real-world applications, and **it can lead to a decrease in accuracy if the features are highly correlated**.\n",
    "\n",
    "\n",
    "- **Zero-frequency problem:** In the case of text classification, if a word is not present in the training set, it will have a probability of zero. This problem is known as the zero-frequency problem and can be addressed using various smoothing techniques.\n",
    "\n",
    "\n",
    "- **Sensitivity to irrelevant features:** **Naive Bayes treats all features equally**, and irrelevant features can dilute the importance of relevant ones. Feature selection or dimensionality reduction techniques can be used to address this issue.\n",
    "\n",
    "\n",
    "- **Data scarcity:** Naive Bayes requires a sufficient amount of data to estimate the probabilities accurately. In cases where data is limited, the estimates can be inaccurate, leading to decreased performance.\n",
    "\n",
    "\n",
    "- **Categorical inputs:** Naive Bayes assumes that the input features are categorical, which can limit its applicability to real-world problems with continuous or mixed-type data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c0369",
   "metadata": {},
   "source": [
    "### iv. How come Naive Bayes works well with the large training sets and a high number of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336df8fb",
   "metadata": {},
   "source": [
    "**Simply because it relies on the probability of each feature given the class.** This makes it computationally efficient and scalable, as it doesn't require expensive operations like matrix inversion or gradient descent.\n",
    "\n",
    "In addition, Naive Bayes assumes that the features are conditionally independent given the class, which means that it doesn't suffer from the curse of dimensionality, where the performance of some machine learning algorithms can degrade as the number of features grows larger. This makes Naive Bayes particularly well-suited for high-dimensional datasets, where other algorithms may struggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799974eb",
   "metadata": {},
   "source": [
    "### v. Why doesn't Naive Bayes' require Feature Scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab970579",
   "metadata": {},
   "source": [
    "In Naive Bayes, the algorithm calculates the probabilities of each feature based on the training data, and then uses these probabilities to make predictions on new data. **The algorithm does not depend on the magnitude or scale of the features, but only on their probability distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192da68",
   "metadata": {},
   "source": [
    "### vi. What's the impact of Outliers and Missing Values on Naive Bayes'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2577f",
   "metadata": {},
   "source": [
    "- ### Impact of Outliers\n",
    "    Naive Bayes algorithm works on probability to predict the outcome of a classification problem. Outliers are data points that are far away from the majority of the data points, and **they can significantly affect the calculation of probabilities in the algorithm**. **If outliers are present in the data, they can distort the probability distribution and lead to incorrect predictions.**\n",
    "    \n",
    "    **In general, outliers can have a significant impact on the performance of Naive Bayes, particularly if they contain features that are not typical of the class they are being classified into. In such cases, the model may assign a higher probability to the outlier than it should, leading to incorrect classifications.** And this could very well lead to **Overfitting**.\n",
    "    \n",
    "    Therefore, it is important to identify and handle outliers appropriately before applying the Naive Bayes algorithm to ensure accurate predictions.\n",
    "        \n",
    "    To mitigate the impact of outliers on NB, it is recommended to preprocess the data by removing or correcting outliers before training the model. Alternatively, **one can use robust variants of Naive Bayes**, such as **Robust Bayesian Network (RBN)** or **Robust Naive Bayes (RNB)**, which are less sensitive to outliers and can handle noisy data better than traditional Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd1376",
   "metadata": {},
   "source": [
    "- ### Impact of Missing Values\n",
    "    Typically, Naive Bayes can simply ignore the record containing the missing value. It usually handle missing values by either removing the instances with missing values or imputing the missing values.\n",
    "\n",
    "    Having said that, **missing values CAN impact the performance of Naive Bayes algorithms, as they rely on the availability of data to calculate probabilities.** When a feature has missing values, it becomes difficult to calculate the probabilities of that feature for a given class label.\n",
    "\n",
    "    If instances with missing values are removed, it can result in a **smaller training dataset and potential loss of valuable information**. Imputing missing values can help to retain the instances and potentially reduce the impact of missing values on the algorithm's performance. However, the choice of imputation method can also affect the performance of the algorithm. Some popular imputation methods include mean imputation, median imputation, and mode imputation. **It's important to note that imputing missing values assumes that the values are missing at random and not related to the class labels, as this can introduce bias into the model.**\n",
    "    \n",
    "    Overall, the impact of missing values on Naive Bayes depends on **the amount of missing data**, **the method used to handle missing values**, and **the relationship between the missing values and the class labels**. It's important to carefully analyze the dataset and choose an appropriate method to handle missing values to ensure the best performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7e270",
   "metadata": {},
   "source": [
    "### vii. How does NB trade higher bias for lower variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84f63f",
   "metadata": {},
   "source": [
    "The assumption of feature independence can introduce bias into the model, which means that the model may not be able to capture the complex relationships between the features and the target variable. However, this bias also reduces the variance of the model, which means that the model is less likely to overfit the training data and generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eff34a",
   "metadata": {},
   "source": [
    "In simpler terms, Naive Bayes makes assumptions that might not be completely accurate, but it makes these assumptions to keep things simple and efficient. These assumptions introduce some bias into the model, but they also help to reduce the amount of variance in the model.\n",
    "\n",
    "By reducing variance, Naive Bayes is less likely to overfit the training data and is better able to generalize to new data. So, while the assumptions made by Naive Bayes may introduce some bias, the overall result is a model that is more stable and less prone to overfitting, which leads to better performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edbe2de",
   "metadata": {},
   "source": [
    "### viii Explain the intuition of Bias and Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9965902",
   "metadata": {},
   "source": [
    "**When we say that a machine learning model has low bias for a particular dataset, it means that the model is able to capture the underlying patterns and relationships in the data accurately.** In other words, the model is able to fit the training data well and make accurate predictions.\n",
    "\n",
    "Bias refers to the difference between the predicted output of a model and the actual output. **A model with high bias is one that oversimplifies the problem and does not capture the complexity of the data, resulting in underfitting.** On the other hand, a model with low bias is able to capture the complexity of the data and make accurate predictions, without oversimplifying the problem.\n",
    "\n",
    "In general, a model with low bias and low variance is considered to be a good model, as it is able to fit the data accurately and make accurate predictions on new data. However, finding the right balance between bias and variance is a common challenge in machine learning, as increasing the complexity of a model to reduce bias can often lead to overfitting and increased variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
