{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c48201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7366141",
   "metadata": {},
   "source": [
    "# i. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72769f00",
   "metadata": {},
   "source": [
    "**Brief:** Apparently to reach a decision by going through a series of binary decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42abad4",
   "metadata": {},
   "source": [
    "A **Decision Tree** is a popular machine learning algorithm which can be used for classification as well as regression. It is a **tree-like model** in which **internal nodes represent the features or attributes of the data**, **branches represent the decision rules**, and **the leaves represent the final decisions or outcomes**.\n",
    "\n",
    "**Process of Building a Decision Tree:** It involves selecting the best feature (one at a time) recursively to split the data into smaller subsets (and build a tree), based on some metric such as information gain or Gini impurity. Each split creates a new node in the tree, which is connected to its parent node by a branch. The leaf nodes of the tree represent the final decision, such as a classification or a predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35582bd6",
   "metadata": {},
   "source": [
    "**Example:** Say, we have a dataset of customer information for a company, including attributes like age, income, and gender, and we want to use a decision tree to predict whether a customer will make a purchase. \n",
    "\n",
    "- The decision tree algorithm would first look at all the attributes and **select the one that has the highest information gain, which is a measure of how well a feature can split the data into different classes**.<br><br>\n",
    "\n",
    "- Say, that the age attribute has the highest information gain. The algorithm would then split the dataset into subsets based on age, with one branch representing customers under the age of 30 and another branch representing customers over the age of 30.<br><br>\n",
    "\n",
    "- The algorithm would then repeat this process for each branch, selecting the attribute with the highest information gain and splitting the data again, until it reaches the leaf nodes, which represent the final classification of whether the customer will make a purchase or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65275b83",
   "metadata": {},
   "source": [
    "## i.i Explain Entropy, Gini Impurity, and Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400a277",
   "metadata": {},
   "source": [
    "The decision tree algorithm works by splitting the training set into two subsets using a single feature **k** and a threshold **t(k)**. It searches for the pair **(k, t(k))** **that produces the purest subsets (weighted by their size)**. Once the CART algorithm has successfully split the training set into two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops the recursing once it reaches the maximum depth, or if it can't find a split that will reduce impurity.\n",
    "\n",
    "Now to measure the said purity of the subsets divided, the algorithm uses the **Gini** or **Entropy** measures. The Gini Impurity and Entropy measures are used to evaluate the impurity of a single node in the decision tree. **First, the algorithm computes the Gini Impurity or Entropy of the entire dataset for each feature.** It evaluates the impurity of the dataset for all possible threshold values of each feature and **selects the feature and threshold that produce the lowest impurity**. **\"A node is 'pure' if all instances it applies to belong to the same class.\"**\n",
    "\n",
    "### i.i.i Gini Impurity\n",
    "\n",
    "    The Gini Impurity ranges from 0 (when all examples in the dataset belong to the same class) to 0.5 (when the examples are evenly distributed across all classes).\n",
    "    \n",
    "### i.i.ii Entropy\n",
    "\n",
    "Entropy is a **measure of the amount of disorder or uncertainty in a dataset**. It is similar to the Gini Impurity as in to measure the impurity of the subset, with the difference in its computation formula.\n",
    "\n",
    "    The Entropy also ranges from 0 (when all examples in the dataset belong to the same class) to 1 (when the examples are evenly distributed across all classes).\n",
    "\n",
    "### i.i.iii Information Gain\n",
    "\n",
    "Information Gain is nothing but a measure of the difference in entropy or Gini Impurity before and after splitting a dataset. **To compare the impurity of the parent node with the impurity of the child nodes after the split.**\n",
    "\n",
    "### Which one shall we use then, Gini Impurity or Entropy?\n",
    "\n",
    "Most of the time, it does not make a big difference: They lead to similar trees. However, Gini Impurity is slightly faster to compute, so it's a good default (the reason being entropy employs the computation of binary log -- a li'l computation expensive comparitively). Anyway, in any case, we do have cross-validation techniques, we can very well choose one of them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf62109",
   "metadata": {},
   "source": [
    "## i.ii Explain the decision tree working for the \"categorical\" and \"numerical\" features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd900be2",
   "metadata": {},
   "source": [
    "Decision tree involves selecting the best feature (one at a time) recursively to split the data into smaller subsets (and build a tree), based on some metrics. Each split creates a new node in the tree, which is connected to its parent node by a branch. The leaf nodes of the tree represent the final decision, such as a classification or a predicted value.\n",
    "\n",
    "Now if we have **categorical variables**, the algorithm works by splitting the training set into two subsets using a single feature **k** and a threshold **t(k)**. It searches for the pair **(t, t(k))** that produces the purest subsets (weighted by their size), measured by **Gini Impurity** or **Entropy**.\n",
    "\n",
    "And should we have **numerical variables**, **instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE**.\n",
    "\n",
    "To further clarify, when dealing with **categorical variables, the decision tree algorithm will split the data based on the presence or absence of a particular category**. For example, if the input variable is the color of a car and there are three possible colors (red, blue, and green), the algorithm might test whether the car is red or not, and split the data accordingly. For **numerical variables, the algorithm will split the data based on a threshold value**. For example, if the input variable is the price of a product, the algorithm might test whether the price is above or below a certain threshold value, and split the data accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73eebd9",
   "metadata": {},
   "source": [
    "## i.iii State out some scenarios where decision trees work well.\n",
    "\n",
    "- **When the data has a mix of categorical and numerical features**: Decision trees can handle both types of features without the need for extensive preprocessing.<br><br>\n",
    "\n",
    "- **When there are complex relationships between features**: Decision trees can capture complex interactions between features that might be difficult to model with linear regression or other models.<br><br>\n",
    "\n",
    "- **When interpretability is important**: Decision trees are easy to interpret and visualize, which **can be useful in fields like healthcare or finance where decisions need to be explained to stakeholders**.<br><br>\n",
    "\n",
    "- **When the dataset is large**: Decision trees can handle large datasets efficiently, making them a good choice when working with big data.<br><br>\n",
    "\n",
    "- **When the problem is nonlinear**: Decision trees can model nonlinear relationships between features, making them useful in tasks like image or speech recognition where the relationships between inputs can be complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ecd38",
   "metadata": {},
   "source": [
    "## i.iv Understanding regarding the hyperparameter techniques in the decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8d380",
   "metadata": {},
   "source": [
    "Here are some common hyperparameters used in decision trees and techniques for tuning them:\n",
    "\n",
    "- **Max depth**: This hyperparameter controls the maximum depth of the decision tree. A deeper tree can model more complex relationships between features, but may also overfit the training data. **Tuning the max depth can help balance the complexity of the model with its ability to generalize to new data.**<br><br>\n",
    "\n",
    "- **Min samples split**: This hyperparameter controls the **minimum number of samples required to split an internal node of the tree**. If the number of samples at a node is less than `min_samples_split`, the algorithm will not attempt to split the node any further. **This hyperparameter controls the number of samples required to form a new branch at an internal node.**\n",
    "\n",
    "  A higher value can help prevent overfitting, but may also result in a simpler model that does not capture all relevant information in the data.<br><br>\n",
    "\n",
    "- **Min samples leaf**: This hyperparameter controls **the minimum number of samples required to be at a leaf node of the tree, to further continue the splitting**. In other words, if the number of samples at a particular node is less than `min_samples_leaf`, then the algorithm will stop splitting and consider the node as a leaf. **This hyperparameter controls the number of samples required to form a terminal node (i.e., a leaf) of the decision tree.**\n",
    "\n",
    "  A higher value can help prevent overfitting and lead to a simpler model, but may also result in lower accuracy on the training data.<br><br>\n",
    "\n",
    "- **Min weight fraction leaf**: Similar to `min_samples_leaf`, but instead of specifying the minimum number of samples required to be at a leaf node, **it specifies the minimum weighted fraction of the total number of input samples that must be present at a leaf node**.\n",
    "\n",
    "  The purpose of `min_weight_fraction_leaf` is to control the depth of a decision tree by stopping tree growth when the sum of weights of the samples in a leaf node is below a certain threshold. This hyperparameter can be used to prevent overfitting in situations where the input data is imbalanced or where certain classes have much smaller sample sizes than others.<br><br>\n",
    "\n",
    "- **Max leaf nodes**: This hyperparameter limits **the maximum number of leaf nodes in the tree**. This can be useful for preventing overfitting and creating a simpler model, but may also result in a loss of accuracy.<br><br>\n",
    "\n",
    "- **Criterion**: This hyperparameter **determines the metric used to measure the quality of a split**. The two most common metrics are Gini impurity and entropy. Tuning this hyperparameter can affect the balance between bias and variance in the model.<br><br>\n",
    "\n",
    "One common technique for tuning hyperparameters in decision trees is to use grid search or random search. **Grid search involves trying out all possible combinations of hyperparameter values, while random search tries out a random subset of combinations.** Another technique is to use cross-validation, where the data is split into training and validation sets and the hyperparameters are optimized based on the validation set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ad382",
   "metadata": {},
   "source": [
    "## i.v Can decision trees handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec8d36",
   "metadata": {},
   "source": [
    "Yes, Decision trees can handle missing data by using different methods for imputation and splitting nodes. When missing data is encountered during tree construction, the decision tree algorithm can either **impute the missing value**, **ignore the missing value**, or **create a new branch in the tree for instances with missing values**.\n",
    "\n",
    "- **Imputation:** One common approach for imputation is to use the most common value for categorical features or the mean or median for continuous features, **only when the missing data is not systematic and occurs randomly**. Another approach is to use regression imputation or other more complex methods to estimate the missing values based on the available data.<br><br>\n",
    "\n",
    "- **Creating a new branch:** One strategy is to create a separate branch for instances with missing values, and use the remaining data to split other nodes.\n",
    "\n",
    "The choice of strategy depends on the nature of the data and the objectives of the analysis. In summary, the hyperparameters that can be used to handle missing data in decision trees include:\n",
    "\n",
    "- **max_features**: determines the maximum number of features that the algorithm can consider at each split, allowing the exclusion of features with missing values. **By setting this parameter to a value that excludes the features with missing values, the algorithm can ignore the missing values during tree construction.**<br><br>\n",
    "\n",
    "- **missing_values**: specifies how missing values are handled during tree construction, allowing the use of imputation techniques or surrogate splits to create a separate branch for instances with missing values. Can set this parameter to **'ignore'** to ignore missing values, or **'raise'** to raise an error if missing values are encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505348f6",
   "metadata": {},
   "source": [
    "## i.vi Can decision trees overfit to the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1280e82",
   "metadata": {},
   "source": [
    "Yes, decision trees are prone to the overfitting. They are non-parametric algorithms and as such, tend to stick closely to the training data if the tree is deep and too complex.\n",
    "\n",
    "To avoid overfitting, techniques such as **pruning**, **limiting the maximum depth of the tree**, and **using an appropriate minimum number of samples per leaf node** can be used.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b98936",
   "metadata": {},
   "source": [
    "## i.vii State the advantages and disadvantages of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a22db",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "\n",
    "- Decision Tress are **very intuitive** and can be easily understood by both technical and non-technical audiences. The decision rules are represented in a tree-like structure, making it easy to visualize the decision-making process.<br><br>\n",
    "\n",
    "- Decision trees **can handle both numerical and categorical data without the need for data transformation**. This makes them more versatile than other machine learning algorithms that can only handle specific types of data.<br><br>\n",
    "\n",
    "- Decision trees are a **non-parametric method**, which means that they do not make any assumptions about the distribution of the data. This makes them useful for both linear and non-linear data.<br><br>\n",
    "\n",
    "- Decision trees **can handle missing data** by simply ignoring the missing values and splitting the data based on the available values. This makes them more robust to missing data than other machine learning algorithms.<br><br>\n",
    "\n",
    "- **Feature Selection:** Decision trees assign an **importance score** to each feature **based on its contribution to splitting the data**. This score is calculated by measuring the decrease in impurity of the target variable when a feature is used for splitting. Features with higher importance scores are considered more important for predicting the target variable, and can be selected for further analysis.<br><br>\n",
    "\n",
    "-  Decision trees **can handle multi-class problems**, which means that they can classify data into more than two classes. This makes them useful for a wide range of classification problems.<br><br>\n",
    "\n",
    "- Decision trees are **easy to validate** using methods such as cross-validation, which helps to prevent overfitting and improve the generalization performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e6ef9",
   "metadata": {},
   "source": [
    "### Disavantages:\n",
    "\n",
    "- Probablity of overfitting is very high, especially when the tree is deep and complex. This can lead to poor generalization on unseen data.<br><br>\n",
    "\n",
    "- **Instability:** Decision trees **are sensitive to small variations in the data**, and can produce different trees for different samples of data.<br><br>\n",
    "\n",
    "- **Greediness:** Decision trees are greedy in nature, and choose the best split at each node based on the available features, without considering the global optimum (what's gonna happen in upcoming levels). This can lead to suboptimal trees.<br>\n",
    "\n",
    "    It may not lead to the **global optimum**. In other words, it may miss out on certain patterns or correlations in the data that can be found by exploring other options, resulting in suboptimal trees. Therefore, the decision tree may not generalize well on new data and may not be as accurate as it could be.<br><br>    \n",
    "\n",
    "- It takes more time to train a decision tree model than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6c0dc",
   "metadata": {},
   "source": [
    "## i.viii Explain the bias-variance trade-off in the decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ffa64",
   "metadata": {},
   "source": [
    "**Generally, decision trees have \"low bias\" and \"high variance\".** This is because decision trees are capable of fitting complex and non-linear relationships in the data, which gives them low bias. However, this flexibility can also lead to overfitting, which causes high variance.\n",
    "\n",
    "This overfitting can be controlled by pruning the tree or using ensemble methods such as random forests or gradient boosting, which combine multiple decision trees to reduce overfitting and improve performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed595dda",
   "metadata": {},
   "source": [
    "## i.ix Explain the outliers handling in the decision trees.\n",
    "\n",
    "**Decision trees are relatively robust to outliers because they make splits based on purity of data in the node, rather than trying to fit a best-fit line or curve to the data.** Outliers tend to be isolated data points that don't fit well with the rest of the data, **which makes them easy for a decision tree to split away from the rest of the data**. In other words, outliers can lead to split points in the tree that isolate them from the majority of the data, rather than compromising the overall fit of the model. \n",
    "\n",
    "However, it is important to note that decision trees can still be affected by outliers if they are present in large numbers or if they are influential in the outcome variable, so it is still important to preprocess the data and handle outliers appropriately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
