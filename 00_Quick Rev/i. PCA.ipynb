{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94030e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18ceb6",
   "metadata": {},
   "source": [
    "# i. Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ab168",
   "metadata": {},
   "source": [
    "The **Curse of Dimensionality** refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. It gives rise to following problems:\n",
    "\n",
    "**i.** **Increase in Computation time:** Majority of the machine learning algorithms rely on the calculation of distances for model building and as the number of dimensions increases it becomes more and more **computation-intensive** to create a model out of it.\n",
    "\n",
    "\n",
    "**ii.** **Hard (or almost impossible) to visualise the relationship between features:** We as humans are bound by the perception of a max of three dimensions. We can't comprehend things beyond three dimnesions. Say, we have 1000 features in the dataset. That results in a total (1000\\*999)/2= 499500 combinations possible for creating the 2-D graph.\n",
    "\n",
    "Questions that need to be asked at this point:\n",
    "\n",
    "- Are all the features really contributing to decision making?\n",
    "- Is there a way to come to the same conclusion using a lesser number of features?\n",
    "- Is there a way to combine features to create a new feature and drop the old ones?\n",
    "- Is there a way to remodel features in a way to make them visually comprehensible?\n",
    "\n",
    "Dimensionality reduction techniques such as PCA, waltz in as our saviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb9277",
   "metadata": {},
   "source": [
    "# ii PCA (Principal Component Analysis).\n",
    "\n",
    "**PCA (Principal Component Analysis)** is a dimensionality reduction technique -- not an elimination one -- using which we can reduce the number of features to be used for building the models without losing a significant meaning (or variation) of the data. \n",
    "\n",
    "**It essentially reduces the degree of freedom of the dataset avoiding chances of overfitting.** \n",
    "\n",
    "**General Idea:** Here in PCA, the general idea is to take couple of features at a time and workaroud them in such a way that they can be represented by lesser number of features without losing too much variation from the original dataset.\n",
    "\n",
    "There are two approaches to implement PCA: \n",
    "\n",
    "- **SVD (Singular Value Decomposition)**: Here, PCA can be done by the singular value decomposition of a data matrix.\n",
    "\n",
    "\n",
    "- **Eigen-decomposition of Î£**: PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f668a5",
   "metadata": {},
   "source": [
    "## i.i Principal Components\n",
    "\n",
    "**Principal Components** are **derived features which explain the maximum variance in the data**. The first component explains the maximum variance of the data, the second a bit less, and the third a bit lesser and it goes on and on.\n",
    "\n",
    "Each of the new dimensions found using PCA is a linear combination of the old features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c5d61",
   "metadata": {},
   "source": [
    "## i.ii Steps involved in PCA implementing SVD\n",
    "\n",
    "Let's have a look at how two features at a time can be represented by a single one i.e principal components, via PCA using SVD:\n",
    "\n",
    "### Step i.\n",
    "Straight off the bat, PCA is gonna standard normalize our data such that its mean=0 and s.d=1. This is also called  **Parallel Translation** or **Standardization**. **The main reason for normalization is to ensure that features with larger scales do not dominate the analysis, but in some cases this may not be a concern.**\n",
    "\n",
    "\n",
    "### Step ii.\n",
    "Now to find out a single feature representing two features at a time, PCA will find a **kinda \"best fitted line\"** representing features X1 and X2. **It finds the direction in which the data varies the most, and the line in that very direction will be our \"first principal component\"**. \n",
    "\n",
    "### Step iii. \n",
    "Now, to find out that best fitted line, PCA starts off with a line in any initial direction passing through the origin and **iteratively update it to converge to the first principal compomnent** by calculating the **\"summission of squares of distances between the datapoints and their projections on those random lines\"**. **Whichever line will have the least said distances computed altogether, will be finalized as the PC1.**\n",
    "\n",
    "To ease up the calculations, instead of computing the distances between the projections and the datapoints, PCA calculates the distances of those projections from the origin and accordingly deems the PC1 the one with having the maximum said distance from the origin.\n",
    "\n",
    "The variance of the data projected onto PC1, which is equal to the sum of squares of distances between the projections and the origin, is know as **Eigen Value for PC1**. Similarly, the eigenvalue for PC2 is the variance of the data projected onto PC2, and so on.\n",
    "\n",
    "**Trivia**: Square root of Eigen Value for PC1 is referred as **Singular Value of PC1**.\n",
    "\n",
    "### Step iv. \n",
    "Now to find out the **PC2**, a perpendicular plane to PC1 is being drawn through the origin, and quite similarly a plane perpendicular to both PC1 and PC2 will be our **PC3**, and this goes on and on.\n",
    "\n",
    "\n",
    "### Step v. \n",
    "Now, the optimal number of PCs can be chosen based on the variance each PC accounts for. The graph representing the **explained variance ratios** each PC holds on to is referred to as **Scree Plot**.\n",
    "\n",
    "The **\"optimal number of PCs\"** to retain depends on the **specific application** and **the desired trade-off between explained variance and dimensionality reduction**. \n",
    "\n",
    "**NOTE**: The **Scree Plot** can be a useful tool for visualizing the amount of variance explained by each PC, but it is not always clear where to set the cutoff. Other methods, such as cross-validation or information criteria, may be used to determine the optimal number of PCs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
