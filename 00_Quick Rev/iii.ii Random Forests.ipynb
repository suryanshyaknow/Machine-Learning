{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef9f1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03aed35",
   "metadata": {},
   "source": [
    "# ii. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88af2c",
   "metadata": {},
   "source": [
    "## ii.i Explain Ensemble techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b9453",
   "metadata": {},
   "source": [
    "An **Ensemble** in machine learning is a **technique that combines the predictions of multiple models to improve the overall performance of the system**. **The idea behind ensembling is that different models will make different errors, and by combining them, we can reduce the overall error rate and create a more accurate and robust prediction.**\n",
    "\n",
    "There are several types of ensemble techniques, but the most commonly used ones are **bagging** and **boosting**. **Bagging**, **short for bootstrap aggregating**, involves training multiple models independently on different subsets of the training data and combining their predictions by taking the average or majority vote. **This approach helps to reduce the variance in the model and prevent overfitting.** \n",
    "\n",
    "**Boosting**, on the other hand, involves training models sequentially, with each subsequent model learning from the mistakes of the previous ones. **This approach helps to reduce bias and improve the accuracy of the model.**\n",
    "\n",
    "Ensemble methods are particularly useful when the data is noisy or the underlying relationships are complex, and they can help to improve the robustness and reliability of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65e1b3",
   "metadata": {},
   "source": [
    "## ii.ii Explain Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d630e",
   "metadata": {},
   "source": [
    "**Random forests** are versatile machine learning algorithms that can be used for both regression and classification tasks. They are a bagging ensemble of multitude of decision trees that are trained on different subsets of the training data. \n",
    "\n",
    "**During training, the algorithm constructs the desired number of decision trees (a hyperparameter) using different subsets of the training data, allowing it to average the predictions of each decision tree for regression problems, or output the class that gains the most number of votes among all decision trees for classification problems.**\n",
    "\n",
    "The **feature sampling process** in Random Forests helps to reduce overfitting and create a more generalized model by using different subsets of features for each tree. This also helps to reduce the correlation between features, as each tree is built on a different set of features. Additionally, the use of decision trees in Random Forests allows for the model to capture complex non-linear relationships in the data, leading to a low bias.\n",
    "\n",
    "**Overall, the Random Forest algorithm is a powerful and popular machine learning technique that leverages the bias-variance trade-off to create accurate and robust models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f5548",
   "metadata": {},
   "source": [
    "## ii.iii Explain the bias-variance trade-off in Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a29f5",
   "metadata": {},
   "source": [
    "The Random Forest algorithm **trades low bias for low variance**, which is desirable in an ideal model. The use of decision trees in Random Forests allows for the model to capture complex non-linear relationships in the data, leading to a **low bias**. By constructing an ensemble of decision trees, the algorithm is able to reduce the variance of the model and improve its performance on new, unseen data.\n",
    "\n",
    "Each individual decision tree in the ensemble has high variance and high bias (because of the randomly selected subset of featurs, the tree won't be able to capture the complex relationships of the training data, and we are essentially removing some of the potentially important predictors from consideration, which can lead to an oversimplified model with high bias), while **the ensemble as a whole has low variance and low bias**.\n",
    "\n",
    "To achieve this, Random Forests randomly sample both the rows and columns (features) of the dataset to create different subsets of the training data for each decision tree. This helps to reduce overfitting and increase the diversity of the ensemble.\n",
    "\n",
    "During prediction, each decision tree in the ensemble outputs a prediction and the final output of the Random Forest is the average (for regression) or the majority vote (for classification) of all the individual tree predictions. **This process of averaging or voting helps to further reduce the variance of the model and make it more robust to noise and outliers in the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f718b",
   "metadata": {},
   "source": [
    "## ii.iv State the advantages and disdvantages of the Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fbacc",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "\n",
    "- **Accuracy:** Random Forests are known for their high accuracy and have proven to perform well on a wide range of tasks, including classification and regression.<br><br>\n",
    "\n",
    "- **Robustness:** Random Forests are robust to overfitting, thanks to the use of an ensemble of decision trees. By aggregating the predictions of many trees, the noise in the predictions of individual trees is reduced.<br><br>\n",
    "\n",
    "- **Non-parametric:** Random Forests are non-parametric, which means that they don't make assumptions about the distribution of the data. This makes them suitable for a wide range of applications and data types.<br><br>\n",
    "\n",
    "- **Feature Importance/Selection:** Random Forests provide a measure of feature importance, which can help in understanding the most important variables in a dataset.<br><br>\n",
    "\n",
    "- **Scalability:** Random Forests can handle large datasets with high dimensionality and many features.<br><br>\n",
    "\n",
    "- **Outlier Detection:** Random Forests can be used for outlier detection, which is the process of identifying data points that are significantly different from the majority of the data.<br>**Random Forests can detect outliers by evaluating the effect of each individual sample on the overall accuracy of the model.** This is done by measuring the decrease in accuracy of the model when a particular sample is removed. Samples that have a significant impact on the model's accuracy are considered outliers.<br>In a Random Forest model, each decision tree is constructed on a different subset of the data. Therefore, **the impact of an outlier on the overall model accuracy can be evaluated across multiple trees**, making the outlier detection more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a1612",
   "metadata": {},
   "source": [
    "### Disadvantages:\n",
    "\n",
    "- **Non-Interpretability:** As the number of decision trees in the ensemble increases, it becomes more difficult to interpret the results and understand the relationship between the input features and the target variable. This is because the Random Forest model is a black box model, meaning it doesn't provide clear insight into how the model makes its predictions.<br><br>\n",
    "\n",
    "- **Computationally expensive:** Random Forests can be computationally expensive to train, especially when dealing with large datasets or a high number of trees in the ensemble. This can make it difficult to use in real-time applications where speed is critical.<br><br>\n",
    "\n",
    "- **Can overfit:** While Random Forests are generally robust to overfitting, it is still possible for them to overfit the training data, **especially if the number of trees in the ensemble is too high or the hyperparameters are not well-tuned**.<br><br>\n",
    "\n",
    "- **Limited extrapolation ability:** Extrapolation is the process of using a statistical model to make predictions beyond the range of the data used to build the model. In other words, it involves estimating values outside the range of the data by extending a curve or line beyond the observed data points.\n",
    "\n",
    "    Random Forests may not perform well when extrapolating beyond the range of the training data, as they are not designed to model complex interactions between variables outside of their training data.<br><br>\n",
    "\n",
    "- **Difficulty with imbalanced data:** Random Forests may struggle to correctly classify minority classes in imbalanced datasets, as they tend to favor the majority class. This can be mitigated by resampling techniques or adjusting class weights, but it still may be a challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
